Hypothesis Proposal: Towards Viable Human-AI Systems – A Cybernetic Framing Synthesizing Recent LLM Research

Introduction

Recent advancements in Large Language Models (LLMs) have moved beyond their initial conception as sophisticated text generators towards becoming more "agentic" entities capable of complex reasoning, planning, and autonomous action (Lou et al., 2025). While their individual capabilities are rapidly increasing, the effective integration of these agents into collaborative human teams remains a key challenge, often hampered by issues of trust, alignment, and dynamic coordination (Lou et al., 2025). Addressing these challenges necessitates a fundamental rethinking of human-AI interaction, moving from AI-as-tool paradigms to conceptualizing the human-AI collective as a dynamic, integrated system.

Drawing upon contemporary research in LLM capabilities and a foundational cybernetic perspective, we propose a hypothesis for the potential emergence of viable human-AI systems.

Core Hypothesis

We hypothesize that a composite system comprising human agents, agentic Large Language Models (LLMs) equipped with specific internal self-improvement mechanisms, and a supporting runtime platform can function as an independent, autonomous cybernetic system, demonstrating characteristics of viability aligned with the Viable System Model (VSM).

Conceptual Basis and Supporting Research

This hypothesis is predicated on the synthesis of insights from recent research, which collectively suggest the necessary components and dynamics for such a system:

AI Internal State as Contextual Sensing and Internal Modeling (Garcia et al., 2025): Research demonstrates that the internal hidden states of LLMs, particularly during the prefill phase, encode robust, domain-specific information. These "latent domain-related trajectories" provide the AI component with a form of internal contextual awareness, allowing it to differentiate inputs based on implicit characteristics rather than solely superficial features. In a cybernetic system, this aligns with the crucial functions of environmental sensing (Perception) and building an internal model of the situation (Comprehension), which are foundational for effective control and adaptation (Endsley, 1995; VSM System 4 - Intelligence). The ability to leverage this for dynamic routing represents a primitive form of internal resource allocation based on perceived context.

AI Self-Improvement via Recursive Refinement (Costello et al., 2025): Investigations into self-training mechanisms reveal that LLMs can significantly enhance task-specific capabilities, such as reasoning, by iteratively generating their own data, applying correctness filtering, and fine-tuning their weights based on these validated outputs (Think, Prune, Train). This process demonstrates a powerful loop for recursive capability adaptation. While this refinement relies on an external computational process (SFT managed by the runtime), the data generation and validation criteria are internal to the model's logic and the task context. This mechanism represents a critical component for VSM System 3 (Optimization/Management), enabling the system's operational units (VSM System 1) to improve their performance based on feedback (implicitly, correctness).

Human-AI Teaming Frameworks for System Viability (Lou et al., 2025): Comprehensive reviews of human-AI teaming highlight the transition to AI as agentic collaborators and emphasize the need for structured approaches to team formulation, coordination, maintenance, and training. Drawing on Team Situation Awareness (SA) theory and aligning with VSM principles, this work underscores the requirements for shared goals, role specification and fluidity, communication protocols, trust-building, and adaptive learning within the human-AI collective. This framework provides the necessary organizational and socio-technical scaffolding (VSM Systems 1-5) to integrate the sensing capabilities (point 1) and self-improvement mechanisms (point 2) into a cohesive, goal-directed system capable of surviving and adapting in a dynamic environment (Viability). The inclusion of the "Runtime Platform" recognizes the essential computational substrate and meta-processes (like managing the training loop) that enable the AI's agentic and self-improving functions within the team context.

Implications and Call for Review

This hypothesis offers a novel perspective on human-AI interaction, framing the composite human-AI entity as a subject of cybernetic analysis. It moves beyond viewing AI as a mere tool or focusing solely on the individual AI agent's capabilities. However, validating and elaborating this hypothesis requires rigorous academic inquiry into several key areas:

Methodological Challenges: How can we empirically measure the internal state-based contextual understanding of AI agents and its impact on team SA? What methodologies are appropriate for evaluating the contribution of AI's self-improvement loops to the overall team's performance and adaptability? How can the complex feedback loops between human actions, AI outputs, AI self-tuning, and runtime management be quantified and modeled? Evaluating the "viability" of such a complex, hybrid system poses significant measurement challenges across computational, psychological, and organizational levels.

Theoretical Grounding: How deeply do the observed "latent domain-related trajectories" map to human-like conceptual understanding, and what are the theoretical implications for shared mental models in human-AI teams? How does the recursive self-tuning process relate to established theories of learning and adaptation in complex systems? Does the reliance on an external (albeit self-directed) training process preclude the application of stricter cybernetic concepts like Autopoiesis, and what does this boundary imply for the system's identity and autonomy?

System Architecture and Design: What are the optimal architectural designs for a "Runtime Platform" that facilitates the seamless integration of AI internal states, self-improvement loops, and human interaction protocols required for VSM functions? How can role specification and fluidity be computationally managed and aligned with human cognitive and social dynamics?

Ethical and Governance Implications: Framing the human-AI collective as an autonomous cybernetic system raises profound questions about responsibility, accountability, and control at the system level, extending beyond individual AI ethics or human oversight.

We present this hypothesis and its preliminary conceptual basis for critical review and critique by the academic community. We invite professors and researchers across relevant disciplines (Computer Science, Information Systems, Management, Psychology, Organizational Behavior, Cybernetics) to engage with this framework, scrutinize its elements, challenge its assumptions, and help shape the research agenda for understanding and building genuinely viable human-AI systems. Feedback on the theoretical rigor, methodological feasibility, and identified research questions is particularly welcomed as a basis for future collaborative investigation.

References:

Costello, C., Guo, S., Goldie, A., & Mirhoseini, A. (2025). THINK, PRUNE, TRAIN, IMPROVE: SCALING REASONING WITHOUT SCALING MODELS. arXiv preprint arXiv:2504.18116.

Endsley, M. R. (1995). Toward a theory of situation awareness in dynamic systems. Human factors, 37(1), 85-104.

Garcia, M. H., Diaz, D. M., Kyrillidis, A., Rühle, V., Couturier, C., Mallick, A., Sim, R., & Rajmohan, S. (2025). Exploring How LLMs Capture and Represent Domain-Specific Knowledge. arXiv preprint arXiv:2504.16871.

Lou, B., Lu, T., Raghu, T. S., & Zhang, Y. (2025). Unraveling Human-AI Teaming: A Review and Outlook. arXiv preprint arXiv:2504.05755.

Beer, S. (1981). Brain of the firm: The viable system model. John Wiley & Sons.
